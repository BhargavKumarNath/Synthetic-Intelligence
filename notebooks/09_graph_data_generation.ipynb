{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8cc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_NEIGHBORS = 5\n",
    "N_SYNTHETIC_SAMPLES_TO_GENERATE = 250000\n",
    "BATCH_SIZE = 10000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0b0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMBALANCED_DATA_PATH = '../data/01_raw/original_imbalanced.csv'\n",
    "PROCESSED_DATA_DIR = '../data/02_processed/'\n",
    "GRAPH_DRIVEN_SYNTHETIC_PATH = os.path.join(PROCESSED_DATA_DIR, 'graph_driven_synthetic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc4f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 69554 samples with 41 features\n",
      "Preparing data...\n",
      "Minority samples: 3900\n",
      "Majority samples: 44788\n",
      "Numerical features: 30\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "def prepare_data(df_imbalanced: pd.DataFrame, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, list]:\n",
    "    print(\"Preparing data...\")\n",
    "    \n",
    "    # Split data\n",
    "    df_train = df_imbalanced.sample(frac=0.7, random_state=random_state)\n",
    "    df_minority = df_train[df_train['target'] == 1].drop('target', axis=1)\n",
    "    df_majority = df_train[df_train['target'] == 0]\n",
    "    \n",
    "    # Identify numerical columns\n",
    "    numerical_cols = df_minority.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df_minority_scaled = scaler.fit_transform(df_minority[numerical_cols])\n",
    "    \n",
    "    print(f\"Minority samples: {len(df_minority)}\")\n",
    "    print(f\"Majority samples: {len(df_majority)}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "    \n",
    "    return df_minority, df_majority, df_train, df_minority_scaled, numerical_cols\n",
    "\n",
    "# Load data\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "df_imbalanced = pd.read_csv(IMBALANCED_DATA_PATH)\n",
    "print(f\"Loaded {len(df_imbalanced)} samples with {len(df_imbalanced.columns)} features\")\n",
    "\n",
    "df_minority, df_majority, df_train, df_minority_scaled, numerical_cols = prepare_data(df_imbalanced, RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building kNN graph with 5 neighbors...\n",
      "kNN graph built successfully\n"
     ]
    }
   ],
   "source": [
    "# Build kNN Graph\n",
    "def build_knn_graph(df_minority_scaled: np.ndarray, n_neighbors: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    print(f\"Building kNN graph with {n_neighbors} neighbors...\")\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors + 1, algorithm='ball_tree', n_jobs=-1)\n",
    "    nn.fit(df_minority_scaled)\n",
    "    distances, indices = nn.kneighbors(df_minority_scaled)\n",
    "    \n",
    "    print(\"kNN graph built successfully\")\n",
    "    return distances, indices\n",
    "\n",
    "distances, indices = build_knn_graph(df_minority_scaled, N_NEIGHBORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384789eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Samples Batch\n",
    "def generate_synthetic_batch(\n",
    "    df_minority: pd.DataFrame,\n",
    "    indices: np.ndarray,\n",
    "    numerical_cols: list,\n",
    "    batch_size: int,\n",
    "    random_state: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    minority_numerical = df_minority[numerical_cols].values\n",
    "    minority_categorical = df_minority.drop(columns=numerical_cols)\n",
    "    n_minority_samples = len(df_minority)\n",
    "    \n",
    "    random_node_indices = np.random.randint(0, n_minority_samples, size=batch_size)\n",
    "    synthetic_numerical = np.zeros((batch_size, len(numerical_cols)))\n",
    "    categorical_indices = np.zeros(batch_size, dtype=int)\n",
    "    \n",
    "    for i, node_idx in enumerate(random_node_indices):\n",
    "        neighbor_idx = np.random.choice(indices[node_idx][1:])\n",
    "        interpolation_ratio = np.random.rand()\n",
    "        synthetic_numerical[i] = (\n",
    "            minority_numerical[node_idx] * interpolation_ratio + \n",
    "            minority_numerical[neighbor_idx] * (1 - interpolation_ratio)\n",
    "        )\n",
    "        categorical_indices[i] = node_idx\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(synthetic_numerical, columns=numerical_cols)\n",
    "    \n",
    "    if len(minority_categorical.columns) > 0:\n",
    "        categorical_data = minority_categorical.iloc[categorical_indices].reset_index(drop=True)\n",
    "        synthetic_df = pd.concat([synthetic_df, categorical_data], axis=1)\n",
    "    \n",
    "    synthetic_df['target'] = 1\n",
    "    \n",
    "    return synthetic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9913eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 250000 synthetic samples in batches of 10000...\n",
      "Batch 1/25 completed in 0.11s (10000 samples)\n",
      "Batch 2/25 completed in 0.10s (10000 samples)\n",
      "Batch 3/25 completed in 0.09s (10000 samples)\n",
      "Batch 4/25 completed in 0.09s (10000 samples)\n",
      "Batch 5/25 completed in 0.11s (10000 samples)\n",
      "Batch 6/25 completed in 0.09s (10000 samples)\n",
      "Batch 7/25 completed in 0.09s (10000 samples)\n",
      "Batch 8/25 completed in 0.12s (10000 samples)\n",
      "Batch 9/25 completed in 0.09s (10000 samples)\n",
      "Batch 10/25 completed in 0.09s (10000 samples)\n",
      "Batch 11/25 completed in 0.11s (10000 samples)\n",
      "Batch 12/25 completed in 0.09s (10000 samples)\n",
      "Batch 13/25 completed in 0.09s (10000 samples)\n",
      "Batch 14/25 completed in 0.11s (10000 samples)\n",
      "Batch 15/25 completed in 0.08s (10000 samples)\n",
      "Batch 16/25 completed in 0.07s (10000 samples)\n",
      "Batch 17/25 completed in 0.08s (10000 samples)\n",
      "Batch 18/25 completed in 0.08s (10000 samples)\n",
      "Batch 19/25 completed in 0.07s (10000 samples)\n",
      "Batch 20/25 completed in 0.08s (10000 samples)\n",
      "Batch 21/25 completed in 0.07s (10000 samples)\n",
      "Batch 22/25 completed in 0.07s (10000 samples)\n",
      "Batch 23/25 completed in 0.07s (10000 samples)\n",
      "Batch 24/25 completed in 0.08s (10000 samples)\n",
      "Batch 25/25 completed in 0.08s (10000 samples)\n"
     ]
    }
   ],
   "source": [
    "# Generate Synthetic Samples Optimized (Batch Processing)\n",
    "def generate_synthetic_samples_optimized(\n",
    "    df_minority: pd.DataFrame,\n",
    "    df_majority: pd.DataFrame,\n",
    "    df_train: pd.DataFrame,\n",
    "    indices: np.ndarray,\n",
    "    numerical_cols: list,\n",
    "    n_samples: int,\n",
    "    batch_size: int = 10000,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    print(f\"Generating {n_samples} synthetic samples in batches of {batch_size}...\")\n",
    "    \n",
    "    synthetic_batches = []\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        start_time = time.time()\n",
    "        current_batch_size = min(batch_size, n_samples - batch_idx * batch_size)\n",
    "        batch_random_state = random_state + batch_idx\n",
    "        synthetic_batch = generate_synthetic_batch(df_minority, indices, numerical_cols, current_batch_size, batch_random_state)\n",
    "        synthetic_batches.append(synthetic_batch)\n",
    "        print(f\"Batch {batch_idx + 1}/{n_batches} completed in {time.time() - start_time:.2f}s ({current_batch_size} samples)\")\n",
    "    \n",
    "    df_synthetic = pd.concat(synthetic_batches, ignore_index=True)\n",
    "    df_synthetic = df_synthetic[df_train.columns]\n",
    "    df_final = pd.concat([df_majority, df_synthetic], ignore_index=True)\n",
    "    df_final = df_final.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "df_graph_driven = generate_synthetic_samples_optimized(\n",
    "    df_minority, df_majority, df_train, indices, numerical_cols,\n",
    "    N_SYNTHETIC_SAMPLES_TO_GENERATE, BATCH_SIZE, RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f807892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset saved to ../data/02_processed/graph_driven_synthetic.csv\n",
      "\n",
      "============================================================\n",
      "GRAPH-DRIVEN SYNTHETIC DATA GENERATION COMPLETE\n",
      "============================================================\n",
      "Original minority samples: 3,900\n",
      "Generated synthetic samples: 250,000\n",
      "Final dataset composition:\n",
      "  - Minority class (target=1): 250,000 samples\n",
      "  - Majority class (target=0): 44,788 samples\n",
      "  - Total samples: 294,788\n",
      "  - Class ratio: 5.582\n"
     ]
    }
   ],
   "source": [
    "# Save & Inspect Results\n",
    "df_graph_driven.to_csv(GRAPH_DRIVEN_SYNTHETIC_PATH, index=False)\n",
    "print(f\"Synthetic dataset saved to {GRAPH_DRIVEN_SYNTHETIC_PATH}\")\n",
    "\n",
    "original_minority = len(df_minority)\n",
    "final_minority = len(df_graph_driven[df_graph_driven['target'] == 1])\n",
    "final_majority = len(df_graph_driven[df_graph_driven['target'] == 0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRAPH-DRIVEN SYNTHETIC DATA GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original minority samples: {original_minority:,}\")\n",
    "print(f\"Generated synthetic samples: {N_SYNTHETIC_SAMPLES_TO_GENERATE:,}\")\n",
    "print(f\"Final dataset composition:\")\n",
    "print(f\"  - Minority class (target=1): {final_minority:,} samples\")\n",
    "print(f\"  - Majority class (target=0): {final_majority:,} samples\")\n",
    "print(f\"  - Total samples: {len(df_graph_driven):,}\")\n",
    "print(f\"  - Class ratio: {final_minority/final_majority:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
