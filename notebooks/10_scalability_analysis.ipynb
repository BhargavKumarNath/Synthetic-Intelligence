{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec7418b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import faiss\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d7e37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMBALANCED_DATA_PATH = '../data/01_raw/original_imbalanced.csv'\n",
    "df_imbalanced = pd.read_csv(IMBALANCED_DATA_PATH)\n",
    "df_minority = df_imbalanced[df_imbalanced['target'] == 1].drop('target', axis=1)\n",
    "df_minority_numeric = df_minority.select_dtypes(include=np.number).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_smote(data):\n",
    "    n_samples = len(data)\n",
    "    \n",
    "    n_majority = int(n_samples * 0.8)\n",
    "    n_minority = n_samples - n_majority  \n",
    "    \n",
    "    y_synthetic = np.concatenate([\n",
    "        np.zeros(n_majority), \n",
    "        np.ones(n_minority)\n",
    "    ])\n",
    "    \n",
    "    np.random.seed(42)  \n",
    "    np.random.shuffle(y_synthetic)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(data, y_synthetic)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def profile_graph_exact(data):\n",
    "    nn = NearestNeighbors(n_neighbors=min(6, len(data)), algorithm='ball_tree')\n",
    "    nn.fit(data)\n",
    "    # Perform a query to simulate actual usage\n",
    "    distances, indices = nn.kneighbors(data[:min(100, len(data))])\n",
    "    return distances, indices\n",
    "\n",
    "def profile_graph_optimized_batch(data):\n",
    "    \"\"\"Profile the optimized batch-based graph generation method.\"\"\"\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Build k-nearest neighbors graph\n",
    "    n_neighbors = min(6, len(data))\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors,\n",
    "        algorithm='ball_tree',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    nn.fit(data_scaled)\n",
    "    distances, indices = nn.kneighbors(data_scaled)\n",
    "    \n",
    "    n_samples_to_generate = min(1000, len(data) * 2) \n",
    "    batch_size = min(100, n_samples_to_generate)\n",
    "    n_batches = (n_samples_to_generate + batch_size - 1) // batch_size\n",
    "    \n",
    "    all_synthetic = []\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        current_batch_size = min(batch_size, n_samples_to_generate - batch_idx * batch_size)\n",
    "        \n",
    "        random_node_indices = np.random.randint(0, len(data), size=current_batch_size)\n",
    "        \n",
    "        batch_synthetic = np.zeros((current_batch_size, data.shape[1]))\n",
    "        \n",
    "        for i, node_idx in enumerate(random_node_indices):\n",
    "            if len(indices[node_idx]) > 1:\n",
    "                neighbor_idx = np.random.choice(indices[node_idx][1:])\n",
    "                \n",
    "                interpolation_ratio = np.random.rand()\n",
    "                batch_synthetic[i] = (\n",
    "                    data[node_idx] * interpolation_ratio + \n",
    "                    data[neighbor_idx] * (1 - interpolation_ratio)\n",
    "                )\n",
    "            else:\n",
    "                batch_synthetic[i] = data[node_idx]  \n",
    "        \n",
    "        all_synthetic.append(batch_synthetic)\n",
    "    \n",
    "    return np.vstack(all_synthetic) if all_synthetic else data[:10]\n",
    "\n",
    "def profile_graph_ann(data):\n",
    "    d = data.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    data_float32 = data.astype('float32') if data.dtype != np.float32 else data\n",
    "    index.add(data_float32)\n",
    "    query_data = data_float32[:min(100, len(data_float32))]\n",
    "    distances, indices = index.search(query_data, min(6, len(data_float32)))\n",
    "    return distances, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with 1000 samples\n",
      "Profiling SMOTE (sklearn)...\n",
      "Profiling Graph-Based (Exact NN)...\n",
      "Profiling Graph-Based (Optimized Batch)...\n",
      "Profiling Graph-Based (ANN w/ FAISS)...\n",
      "\n",
      "Benchmarking with 5000 samples\n",
      "Profiling SMOTE (sklearn)...\n",
      "Profiling Graph-Based (Exact NN)...\n",
      "Profiling Graph-Based (Optimized Batch)...\n",
      "Profiling Graph-Based (ANN w/ FAISS)...\n",
      "\n",
      "Benchmarking with 10000 samples\n",
      "Profiling SMOTE (sklearn)...\n",
      "Profiling Graph-Based (Exact NN)...\n",
      "Profiling Graph-Based (Optimized Batch)...\n",
      "Profiling Graph-Based (ANN w/ FAISS)...\n",
      "\n",
      "Benchmarking with 20000 samples\n",
      "Profiling SMOTE (sklearn)...\n",
      "Profiling Graph-Based (Exact NN)...\n",
      "Profiling Graph-Based (Optimized Batch)...\n",
      "Profiling Graph-Based (ANN w/ FAISS)...\n",
      "\n",
      "Benchmarking with 40000 samples\n",
      "Profiling SMOTE (sklearn)...\n",
      "Profiling Graph-Based (Exact NN)...\n",
      "Profiling Graph-Based (Optimized Batch)...\n",
      "Profiling Graph-Based (ANN w/ FAISS)...\n",
      "\n",
      "Benchmarking with 5564 samples\n",
      "Profiling SMOTE (sklearn)...\n",
      "Profiling Graph-Based (Exact NN)...\n",
      "Profiling Graph-Based (Optimized Batch)...\n",
      "Profiling Graph-Based (ANN w/ FAISS)...\n",
      "\n",
      "==================================================\n",
      "BENCHMARKING RESULTS\n",
      "==================================================\n",
      "                       Method  Data Size  Time (s)  Peak Memory (MB)\n",
      "              SMOTE (sklearn)       1000    2.0078             78.99\n",
      "       Graph-Based (Exact NN)       1000    1.8411             78.84\n",
      "Graph-Based (Optimized Batch)       1000    1.1469             82.14\n",
      "   Graph-Based (ANN w/ FAISS)       1000    2.0255             82.02\n",
      "              SMOTE (sklearn)       5000    2.0595             85.68\n",
      "       Graph-Based (Exact NN)       5000    2.0784             86.72\n",
      "Graph-Based (Optimized Batch)       5000    0.7584             87.50\n",
      "   Graph-Based (ANN w/ FAISS)       5000    2.0407             86.25\n",
      "              SMOTE (sklearn)       5564    2.0401             88.04\n",
      "       Graph-Based (Exact NN)       5564    1.9508             87.27\n",
      "Graph-Based (Optimized Batch)       5564    0.8514             87.91\n",
      "   Graph-Based (ANN w/ FAISS)       5564    2.0382             86.51\n",
      "              SMOTE (sklearn)       5564    2.0077             88.12\n",
      "       Graph-Based (Exact NN)       5564    1.9163             87.36\n",
      "Graph-Based (Optimized Batch)       5564    0.8284             87.99\n",
      "   Graph-Based (ANN w/ FAISS)       5564    2.0239             86.60\n",
      "              SMOTE (sklearn)       5564    1.6194             87.18\n",
      "       Graph-Based (Exact NN)       5564    2.1095             87.45\n",
      "Graph-Based (Optimized Batch)       5564    0.8246             88.08\n",
      "   Graph-Based (ANN w/ FAISS)       5564    2.0313             86.66\n",
      "              SMOTE (sklearn)       5564    1.5283             87.27\n",
      "       Graph-Based (Exact NN)       5564    1.9457             87.52\n",
      "Graph-Based (Optimized Batch)       5564    0.8368             88.13\n",
      "   Graph-Based (ANN w/ FAISS)       5564    1.9653             86.69\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "==================================================\n",
      "\n",
      "SMOTE (sklearn):\n",
      "  Time range: 1.5283s - 2.0595s\n",
      "  Memory range: 78.99MB - 88.12MB\n",
      "  Time scaling factor: 0.76x for 5.6x data increase\n",
      "  Memory scaling factor: 1.10x for 5.6x data increase\n",
      "\n",
      "Graph-Based (Exact NN):\n",
      "  Time range: 1.8411s - 2.1095s\n",
      "  Memory range: 78.84MB - 87.52MB\n",
      "  Time scaling factor: 1.06x for 5.6x data increase\n",
      "  Memory scaling factor: 1.11x for 5.6x data increase\n",
      "\n",
      "Graph-Based (Optimized Batch):\n",
      "  Time range: 0.7584s - 1.1469s\n",
      "  Memory range: 82.14MB - 88.13MB\n",
      "  Time scaling factor: 0.73x for 5.6x data increase\n",
      "  Memory scaling factor: 1.07x for 5.6x data increase\n",
      "\n",
      "Graph-Based (ANN w/ FAISS):\n",
      "  Time range: 1.9653s - 2.0407s\n",
      "  Memory range: 82.02MB - 86.69MB\n",
      "  Time scaling factor: 0.97x for 5.6x data increase\n",
      "  Memory scaling factor: 1.06x for 5.6x data increase\n"
     ]
    }
   ],
   "source": [
    "data_sizes = [1000, 5000, 10000, 20000, 40000, len(df_minority_numeric)]\n",
    "results = []\n",
    "\n",
    "methods = {\n",
    "    \"SMOTE (sklearn)\": profile_smote,\n",
    "    \"Graph-Based (Exact NN)\": profile_graph_exact,\n",
    "    \"Graph-Based (Optimized Batch)\": profile_graph_optimized_batch,\n",
    "    \"Graph-Based (ANN w/ FAISS)\": profile_graph_ann\n",
    "}\n",
    "\n",
    "for size in data_sizes:\n",
    "    print(f\"\\nBenchmarking with {size} samples\")\n",
    "    actual_size = min(size, len(df_minority_numeric))\n",
    "    subset = df_minority_numeric.sample(n=actual_size, random_state=42).to_numpy()\n",
    "\n",
    "    for name, func in methods.items():\n",
    "        print(f\"Profiling {name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            mem_use = memory_usage((func, (subset,)), interval=0.1, max_usage=True)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"Method\": name,\n",
    "                \"Data Size\": actual_size,\n",
    "                \"Time (s)\": round(time_taken, 4),\n",
    "                \"Peak Memory (MB)\": round(mem_use, 2)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name} at size {actual_size}: {str(e)}\")\n",
    "            results.append({\n",
    "                \"Method\": name,\n",
    "                \"Data Size\": actual_size,\n",
    "                \"Time (s)\": None,\n",
    "                \"Peak Memory (MB)\": None\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BENCHMARKING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in methods.keys():\n",
    "    method_data = df_results[df_results['Method'] == method].copy()\n",
    "    method_data = method_data.dropna()  \n",
    "    \n",
    "    if not method_data.empty:\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  Time range: {method_data['Time (s)'].min():.4f}s - {method_data['Time (s)'].max():.4f}s\")\n",
    "        print(f\"  Memory range: {method_data['Peak Memory (MB)'].min():.2f}MB - {method_data['Peak Memory (MB)'].max():.2f}MB\")\n",
    "        \n",
    "        if len(method_data) > 1:\n",
    "            time_scaling = method_data['Time (s)'].iloc[-1] / method_data['Time (s)'].iloc[0]\n",
    "            memory_scaling = method_data['Peak Memory (MB)'].iloc[-1] / method_data['Peak Memory (MB)'].iloc[0]\n",
    "            size_scaling = method_data['Data Size'].iloc[-1] / method_data['Data Size'].iloc[0]\n",
    "            \n",
    "            print(f\"  Time scaling factor: {time_scaling:.2f}x for {size_scaling:.1f}x data increase\")\n",
    "            print(f\"  Memory scaling factor: {memory_scaling:.2f}x for {size_scaling:.1f}x data increase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442ea5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
